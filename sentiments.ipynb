{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbPfbcHKSPSU"
      },
      "source": [
        "# Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RnK7_ImkSj_",
        "outputId": "dfa131f1-a3c7-4a4b-f910-5770d25dcbf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting googletrans\n",
            "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (2023.11.17)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans)\n",
            "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (1.3.0)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15715 sha256=ea8a149e0fd92c21326812d6a9251d3cbf9eed85c8b27d66910aa563d281978e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/81/ea/8b030407f8ebfc2f857814e086bb22ca2d4fea1a7be63652ab\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.6\n",
            "    Uninstalling idna-3.6:\n",
            "      Successfully uninstalled idna-3.6\n",
            "Successfully installed chardet-3.0.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=f44e050d9c5c75b90a0b20a225c08b0ce83e0d988beaabd6a7c9df3318ab1409\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "32036\n",
            "1000\n",
            "0\n",
            "review_description    False\n",
            "rating                False\n",
            "dtype: bool\n",
            "0\n",
            "ID                    False\n",
            "review_description    False\n",
            "dtype: bool\n",
            "                                  review_description  rating\n",
            "0  شركه زباله و سواقين بتبرشم و مفيش حتي رقم للشك...      -1\n",
            "1  خدمة الدفع عن طريق الكي نت توقفت عندي اصبح فقط...       1\n",
            "2  تطبيق غبي و جاري حذفه ، عاملين اكواد خصم و لما...      -1\n",
            "3  فعلا تطبيق ممتاز بس لو فى امكانية يتيح لمستخدم...       1\n",
            "4  سيء جدا ، اسعار رسوم التوصيل لا تمت للواقع ب ص...      -1\n",
            "   ID                                 review_description\n",
            "0   1  اهنئكم على خدمه العملاء في المحادثه المباشره م...\n",
            "1   2  ممتاز جدا ولكن اتمنى ان تكون هناك بعض المسابقا...\n",
            "2   3    كل محملته يقول تم ايقاف حطيت2 عشان تسوون الخطاء\n",
            "3   4                                            شغل طيب\n",
            "4   5                                         بعد ماجربت\n",
            "                 ***************** فارع        ****************\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import seaborn as sns\n",
        "from sklearn import preprocessing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import Counter\n",
        "import re\n",
        "import string\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib import rcParams\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import preprocessing\n",
        "import seaborn as sns\n",
        "#load data done\n",
        "import os\n",
        "\n",
        "!pip install googletrans\n",
        "!pip install langdetect\n",
        "\n",
        "from googletrans import Translator\n",
        "from langdetect import detect\n",
        "\n",
        "#display data\n",
        "column_names = [\"review_description\", \"rating\"]\n",
        "train_data = pd.read_excel(\"/content/train.xlsx\", names=column_names)\n",
        "test_data = pd.read_csv(\"/content/test _no_label.csv\",encoding=\"utf-8\",dtype={'review_description': str})\n",
        "print(len(train_data))\n",
        "print(len(test_data))\n",
        "\n",
        "train_data[train_data.isnull().any(axis=1)].head()\n",
        "test_data[train_data.isnull().any(axis=1)].head()\n",
        "print(np.sum(train_data.isnull().any(axis=1)))   #done\n",
        "print(train_data.isnull().any(axis=0))\n",
        "\n",
        "print(np.sum(test_data.isnull().any(axis=1)))   #done\n",
        "print(test_data.isnull().any(axis=0))\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "#display(train_data)\n",
        "#display(test_data)\n",
        "\n",
        "#print(train_data.head())\n",
        "#print(test_data.head())\n",
        "#print(\"***************************************************************************\")\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].astype(str)\n",
        "test_data[\"review_description\"] = test_data[\"review_description\"].astype(str)\n",
        "\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].fillna({\"review_description\":\"مجهول\"})\n",
        "#######################################  NOTE  #############################################################\n",
        "test_data[\"review_description\"]  =test_data[\"review_description\"].fillna({\"review_description\":\"مجهول\"})\n",
        "\n",
        "print(train_data.head())\n",
        "print(test_data.head())\n",
        "\n",
        "for letter in '#.][!XR':\n",
        "    train_data[\"review_description\"] = train_data[\"review_description\"].astype(str).str.replace(letter,'')\n",
        "    test_data[\"review_description\"] = test_data[\"review_description\"].astype(str).str.replace(letter,'')\n",
        "\n",
        "train_data.replace('', np.nan, inplace=True)\n",
        "\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].fillna({\"review_description\":\"مجهول\"})\n",
        "print(\"                 ***************** فارع        ****************\")\n",
        "print(train_data[\"review_description\"].isnull().sum())\n",
        "#train_data.dropna(inplace=True)\n",
        "#print(len(train_data))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_dl0cFzSb3a"
      },
      "source": [
        "# preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQTyEkTPnhO8",
        "outputId": "63939b4b-e5a8-41c3-bb4e-b6b2c0bc73f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emojis in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.9.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.10/dist-packages (0.6.15)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from pyarabic) (1.16.0)\n",
            "              *****************    فارع        ****************\n",
            "0\n",
            "                 ***************** فارع        ****************\n",
            "0\n",
            "                 ***************** فارع        ****************\n",
            "0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0        شركه زباله سواقين بتبرشم مفيش حتي رقم لشكاوي ا...\n",
              "1        خدمه الدفع طريق الكي نت توقفت عندي اصبح فقط ال...\n",
              "2        تطبيق غبي جاري حذفه عاملين اكواد خصم نستخدمها ...\n",
              "3        فعلا تطبيق متاز امكانيه يتيح لمستخدم التطبيق ا...\n",
              "4              سيء جدا اسعار رسوم التوصيل لا تمت لواقع صله\n",
              "                               ...                        \n",
              "32031    التطبيق اصبح سيء لغايه نقوم بطلب لا يتم وصول ا...\n",
              "32032                                           y love you\n",
              "32033                 الباقه بتخلص وبشحن مرتين باقه اضافيه\n",
              "32034    تطبيق فاشل وصلني الطلب ناقص ومش ينفع اعمل حاجه...\n",
              "32035                                      ليش ما يفتح معي\n",
              "Name: review_description, Length: 32036, dtype: object"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#preprocessing\n",
        "#from translate import Translator\n",
        "from googletrans import Translator\n",
        "import nltk\n",
        "from googletrans import Translator\n",
        "#from translate import Translator\n",
        "#done but with limimted trials\n",
        "def translate_EtoA(english_text):\n",
        "    # Create an instance of the Translator\n",
        "    translator = Translator(service_urls=['translate.google.com'],to_lang=\"ar\")\n",
        "    # Translate English text to Arabic\n",
        "    translation = translator.translate(english_text)\n",
        "    # Access the translated text\n",
        "    arabic_text = translation\n",
        "    return arabic_text\n",
        "\n",
        "#!pip install emojis\n",
        "#!pip install emoji\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import csv\n",
        "!pip install emojis\n",
        "!pip install emoji\n",
        "import emojis\n",
        "import emoji\n",
        "nltk.download('stopwords')\n",
        "\n",
        "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
        "english_punctuations = string.punctuation\n",
        "punctuations_list = arabic_punctuations + english_punctuations\n",
        "\n",
        "def remove_punctuations(text):\n",
        "    translator = str.maketrans('', '', punctuations_list)\n",
        "    return text.translate(translator)\n",
        "\n",
        "def remove_repeating_char(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    stop_words = set(stopwords.words('arabic')) - {'نعم','لا', 'ليس', 'ليست', 'مش', 'ما','غير','أقبل','ليس','ليسا','ليست','ليستا','ليسوا','لست','لستم','لستما','لستن','لسن','لسنا','واو'}\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return \" \".join([word for word in tokens if word not in stop_words])\n",
        "\n",
        "def build_emoji_dictionary(csv_file):\n",
        "    emoji_dict = {}\n",
        "    with open(csv_file, 'r', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        for row in reader:\n",
        "            emoji = row[0]\n",
        "            text = row[1]\n",
        "            emoji_dict[emoji] = text\n",
        "    return emoji_dict\n",
        "\n",
        "def replace_emojis(emoji_dict, sentence):\n",
        "    emojis = emoji.emoji_list(sentence)\n",
        "    for emo in emojis:\n",
        "        if emo['emoji'] in emoji_dict:\n",
        "            # Replace the emoji with the corresponding text surrounded by asterisks\n",
        "            sentence = sentence.replace(emo['emoji'], '*' + emoji_dict[emo['emoji']] + '* ')\n",
        "    return sentence\n",
        "\n",
        "def remove_emojis(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "def decode_emojis(text):\n",
        "    # Replace emojis with Arabic words\n",
        "    decoded_text = emojis.decode(text)\n",
        "    return decoded_text\n",
        "\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "    return text\n",
        "\n",
        "emoji_dict = build_emoji_dictionary(\"/content/emojis.csv\") #Assuming the CSV file is in the same directory\n",
        "# in Arabic syllabels are represented by small signs above or below\n",
        "# each character, we will remove them\n",
        "# and see if they affect our prediction model\n",
        "!pip install pyarabic\n",
        "import pyarabic.araby as araby\n",
        "\n",
        "def remove_diactrics(text):\n",
        "    return araby.strip_diacritics(text)\n",
        "\n",
        "def processPost(text,emoji_dict):\n",
        "    #Replace @username with empty string\n",
        "    text = re.sub(r\"\\s+\", \" \", str(text).strip())\n",
        "\n",
        "    text = re.sub('@[^\\s]+', ' ', text)\n",
        "    #Convert www.* or https?://* to \" \"\n",
        "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',text)\n",
        "    #remove nums\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    #Replace #word with word\n",
        "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
        "    # remove punctuations\n",
        "    text= remove_punctuations(text)\n",
        "    text = normalize_arabic(text)\n",
        "    #handle emojes\n",
        "    text = replace_emojis(emoji_dict, text)\n",
        "\n",
        "    text= remove_punctuations(text)\n",
        "    # Translate English to Arabic\n",
        "    #text = translate_EtoA(text)  done but with limimted trials\n",
        "\n",
        "    # remove repeated letters\n",
        "    text = remove_repeating_char(text)\n",
        "    text = remove_stop_words(text)\n",
        "    #print(text)\n",
        "    text = remove_diactrics(text)\n",
        "    return text\n",
        "\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].fillna({\"review_description\":\"مجهول\"})\n",
        "print(\"              *****************    فارع        ****************\")\n",
        "print(train_data[\"review_description\"].isnull().sum())\n",
        "#print(train_data.shape[0])\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].apply(lambda x: processPost(x,emoji_dict))\n",
        "test_data[\"review_description\"] = test_data[\"review_description\"].apply(lambda x: processPost(x,emoji_dict))\n",
        "\n",
        "\n",
        "train_data.replace('', np.nan, inplace=True)\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].fillna({\"review_description\":\"مجهول\"})\n",
        "print(\"                 ***************** فارع        ****************\")\n",
        "print(train_data[\"review_description\"].isnull().sum())\n",
        "#train_data.dropna(inplace=True)\n",
        "#print(len(train_data))\n",
        "\n",
        "test_data.replace('', np.nan, inplace=True)\n",
        "test_data[\"review_description\"] = test_data[\"review_description\"].fillna({\"review_description\":\"مجهول\"})\n",
        "print(\"                 ***************** فارع        ****************\")\n",
        "print(test_data[\"review_description\"].isnull().sum())\n",
        "#test_data.dropna(inplace=True)\n",
        "#print(len(test_data))\n",
        "#6 nana cases to be asked 3 number cases  ,1 case :'نناا'  , cases 'ععععع'\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].apply(str)\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].apply(tokenizer.tokenize)\n",
        "\n",
        "\n",
        "#stemming   to try\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "!pip install nltk\n",
        "stemmer = ISRIStemmer()\n",
        "\n",
        "def stem(text):\n",
        "    stemmed = []\n",
        "    for word in text:\n",
        "        stemmed.append(stemmer.stem(word))\n",
        "    return stemmed\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "def engstem(text):\n",
        "    e_stemmer = SnowballStemmer(\"english\")\n",
        "    stemmed = []\n",
        "    for word in text:\n",
        "        stemmed.append(e_stemmer.stem(word))\n",
        "    return stemmed\n",
        "\n",
        "\n",
        "#train_data[\"review_description\"] = train_data[\"review_description\"].apply(stem)\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].apply(engstem)\n",
        "\n",
        "# Join the stemmed tokens back into a string\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].apply(lambda x: ' '.join(x))\n",
        "\n",
        "train_data[\"review_description\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2YcPbDmTC_8"
      },
      "source": [
        "# Install tensorflow_version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlr9b4Sjkqr1",
        "outputId": "4ac1080e-1b79-4850-b5c1-f671b4d2ef02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "from re import X\n",
        "try:\n",
        "    %tensorflow_version 2.X\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "#!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKpDe64BTdV_"
      },
      "source": [
        "# Tokenization and Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ujd_F5MXpVav"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "#regulatization\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "\n",
        "X_train = train_data['review_description']\n",
        "y_train = train_data['rating']\n",
        "x_test = test_data['review_description'].astype(str).tolist()\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_tokenized = tokenizer.texts_to_sequences(X_train)\n",
        "X_tokenized_test =tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# Padding sequences\n",
        "max_len = max([len(x) for x in X_tokenized])\n",
        "X_padded = pad_sequences(X_tokenized, maxlen=max_len, padding='post')\n",
        "X_test_padded = pad_sequences(X_tokenized_test, maxlen=max_len, padding='post')\n",
        "y_train = np.array(y_train)\n",
        "# Encoding labels\n",
        "y_train_encoded = y_train + 1  # Assuming 3 classes\n",
        "y_train_encoded = to_categorical(y_train, num_classes=3)\n",
        "#print(y_train_encoded)\n",
        "#print(y_train_encoded.argmax(axis=-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ6o-5H5cOG4"
      },
      "source": [
        "# Predict the sentiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MBmwr_yFfr4"
      },
      "outputs": [],
      "source": [
        "#prediction\n",
        "def prediction_test(model):\n",
        "  predictions = model.predict(X_test_padded)\n",
        "  # Convert probabilities to class labels\n",
        "  predicted_classes = predictions.argmax(axis=-1)\n",
        "  predicted_classes = np.vectorize({2: -1, 1: 1, 0: 0}.get)(predicted_classes)  # To shift (0, 1, 2) to (-1, 0, 1)\n",
        "  #predicted_labels = [np.argmax(pred) for pred in predicted_classes]\n",
        "  print(predicted_classes)\n",
        "  return predicted_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-_Vev06UBo-"
      },
      "source": [
        "# Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLipq7JhsjZK"
      },
      "outputs": [],
      "source": [
        "#best accuracy\n",
        "import tensorflow as tf\n",
        "!pip install tensorflow_addons\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "class TransformerModel(tf.keras.Model):\n",
        "    def _init_(self, vocab_size, d_model, num_heads, ff_dim, num_transformer_blocks, output_dim, rate=0.1):\n",
        "        super(TransformerModel, self)._init_()\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.attention = tfa.layers.MultiHeadAttention(head_size=num_heads, num_heads=num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation='relu'),\n",
        "            layers.Dense(d_model),\n",
        "        ])\n",
        "        \n",
        "        self.rnn_layer = layers.GRU(16, return_sequences=True)    # Added GRU  layer\n",
        "\n",
        "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "        self.pooling = layers.GlobalAveragePooling1D()\n",
        "        self.dense = layers.Dense(output_dim, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "      x = self.embedding(inputs)\n",
        "\n",
        "      # Reshape x to (batch_size, sequence_length, num_features)\n",
        "      x = tf.reshape(x, [-1, tf.shape(x)[1], self.embedding.output_dim])\n",
        "\n",
        "      attention_output = self.attention([x, x, x])\n",
        "      x = x + attention_output\n",
        "      x = self.rnn_layer(x)\n",
        "      #x = self.lstm_layer(x)  # Added LSTM layer\n",
        "      x = self.layer_norm1(x)\n",
        "      x = self.ffn(x)\n",
        "      x = self.dropout1(x)\n",
        "      x = x + attention_output\n",
        "      x = self.layer_norm2(x)\n",
        "      x = self.pooling(x)\n",
        "      return self.dense(x)\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import layers\n",
        "!pip install keras_tuner\n",
        "import keras_tuner as kt\n",
        "\n",
        "def build_transformer_model(hp):\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    # num_transformer_blocks = 2\n",
        "    output_dim = 3  # Adjust based on your dataset\n",
        "\n",
        "    # d_model = hp.Int('d_model', min_value=32, max_value=128, step=32)\n",
        "    # num_heads = hp.Int('num_heads', min_value=128, max_value=512, step=128)\n",
        "    # ff_dim = hp.Int('ff_dim', min_value=32, max_value=128, step=32)\n",
        "    # dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
        "\n",
        "    num_transformer_blocks = hp.Int('num_transformer_blocks', min_value=2, max_value=6, step=1)\n",
        "    d_model = hp.Int('d_model', min_value=256, max_value=1024, step=256)\n",
        "    num_heads = hp.Int('num_heads', min_value=4, max_value=12, step=2)\n",
        "    ff_dim = hp.Int('ff_dim', min_value=512, max_value=2048, step=512)\n",
        "\n",
        "    model = TransformerModel(vocab_size, d_model, num_heads, ff_dim, num_transformer_blocks, output_dim, rate=dropout_rate)\n",
        "    # learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
        "    learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop']),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "tuner = kt.RandomSearch(\n",
        "    build_transformer_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='transformer_tuning'\n",
        ")\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "tuner.search(\n",
        "    X_padded, y_train_encoded,\n",
        "    epochs=10,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "# Retrieve the best model and hyperparameters\n",
        "best_model1 = tuner.get_best_models(num_models=1)[0]\n",
        "best_hyperparameters1 = tuner.get_best_hyperparameters()[0]\n",
        "\n",
        "print('Best hyperparameters:', best_hyperparameters1.values)\n",
        "predictions_ct = prediction_test(best_model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuoqZ06ziMy2"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def CreateCompare_files(predicted,file_comparePath,file_create):\n",
        "  # Read the CSV file into a DataFrame\n",
        "  id = pd.read_csv(\"/content/test _no_label.csv\", encoding=\"utf-8\", dtype={'ID': int})\n",
        "  print(len(id))\n",
        "  # Assuming you have a column named 'ID' and 'Predicted_Label' in the DataFrame 'id'\n",
        "  with open(\"test_with_label.csv\", mode=\"w\", newline=\"\") as csvfile:\n",
        "    fieldnames = [\"ID\", \"rating\"]\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for index, row in id.iterrows():\n",
        "      writer.writerow({\"ID\": row[\"ID\"], \"rating\": predicted[index]})\n",
        "  y_eval=pd.read_csv(file_comparePath, encoding=\"utf-8\", dtype={'rating': int})\n",
        "  accuracy = accuracy_score(y_eval[\"rating\"],predicted_classes)\n",
        "  precision = precision_score(y_eval[\"rating\"],predicted_classes, average='weighted')\n",
        "  recall = recall_score(y_eval[\"rating\"],predicted_classes, average='weighted')\n",
        "  f1 = f1_score(y_eval[\"rating\"],predicted_classes, average='weighted')\n",
        "  # Print the evaluation metrics\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "  print(\"Precision:\", precision)\n",
        "  print(\"Recall:\", recall)\n",
        "  print(\"F1-score:\", f1)\n",
        "\n",
        "#file best accuray \"/content/test_with_label (2).csv\" to compare\n",
        "#file accuray \"test_with_label.csv\" to create\n",
        "\n",
        "CreateCompare_files(predictions_ct,\"/content/test_with_label (2).csv\",\"test_with_label.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYhrm2-3YZ_j"
      },
      "source": [
        "# LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vluGJgYlUA28"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "!pip install keras_tuner\n",
        "import keras_tuner as kt\n",
        "# Prepare the LSTM model\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 500\n",
        "\n",
        "# Convert labels to categorical\n",
        "\n",
        "# Define your LSTM model\n",
        "def build_lstm_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=500, input_length=max_len))\n",
        "    model.add(LSTM(units=hp.Int('units', min_value=32, max_value=128, step=32), return_sequences=True))\n",
        "    model.add(Dropout(rate=hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "    model.add(Bidirectional(LSTM(units=hp.Int('units_bidi', min_value=32, max_value=128, step=32))))\n",
        "    model.add(Dropout(rate=hp.Float('dropout_bidi', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "\n",
        "    # New Dense layers with hyperparameter tuning\n",
        "    model.add(Dense(units=hp.Int('dense_units_1', min_value=8, max_value=64, step=8), activation=hp.Choice('dense_activation_1', values=['relu', 'sigmoid'])))\n",
        "    model.add(Dropout(rate=hp.Float('dense_dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "\n",
        "    model.add(Dense(units=hp.Int('dense_units_2', min_value=4, max_value=32, step=4), activation=hp.Choice('dense_activation_2', values=['relu', 'sigmoid'])))\n",
        "    model.add(Dropout(rate=hp.Float('dense_dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "tuner = kt.RandomSearch(\n",
        "    build_lstm_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='lstm_tuning'\n",
        ")\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "tuner.search(\n",
        "    X_padded, y_train_encoded,\n",
        "    epochs=10,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "\n",
        "# Retrieve the best model from tuner\n",
        "best_lstm_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
        "\n",
        "print('Best hyperparameters:', best_hyperparameters.values)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "U2YcPbDmTC_8"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
